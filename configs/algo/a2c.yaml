_target_: chunkgfn.algo.a2c.A2C

optimizer:
  _target_: torch.optim.Adam
  _partial_: true
  weight_decay: 0.0

scheduler: null

forward_policy: null
  
critic: null

action_embedder:
  _target_: chunkgfn.models.action_encoder.ActionModel
  n_primitive_actions: 3
  hidden_dim: 128
  action_embedding_dimension: 128

forward_policy_lr: 1e-4
critic_lr: 1e-4
action_embedder_lr: 1e-4
monitor: train/logreward

n_trajectories: 1 # Number of test trajectories to sample
reward_temperature: 0.33333 # Temperature for the reward
n_onpolicy_samples: 512 # Number of on-policy samples to draw for each validation step
entropy_coeff: 0.01 # Entropy coefficient for the policy

chunk_algorithm: bpe
chunk_type: replacement
library_update_frequency: 2
total_library_size: 20
n_samples: 5000