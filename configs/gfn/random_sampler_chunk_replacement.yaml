_target_: chunkgfn.gfn.random_sampler_chunk_replacement.RandomSamplerChunkReplacement

optimizer:
  _target_: torch.optim.Adam
  _partial_: true
  weight_decay: 0.0

replay_buffer:
  _target_: chunkgfn.replay_buffer.prioritized_replay.PrioritizedReplay
  capacity: 10000
  is_conditional: False
  cutoff_distance: 1.4


monitor: train/logreward
ratio_from_replay_buffer: 0.55 # Ratio of samples from replay buffer
reward_temperature: 0.3333 # Temperature for the reward
n_onpolicy_samples: 512 # Number of on-policy samples to draw for each validation step

library_update_frequency: 25 # Frequency of updating the library
n_samples: 10000 # Number of samples to draw from the library
total_library_size: 20 # The size of the library
chunk_algorithm: "bpe" # Algorithm to select chunks, can be either "uniform" or "bpe"