_target_: chunkgfn.gfn.cond_tb_gfn.Cond_TBGFN

optimizer:
  _target_: torch.optim.Adam
  _partial_: true
  weight_decay: 0.0

scheduler: null

forward_model:
  _target_: chunkgfn.models.rnn.RNN
  num_layers: 1
  hidden_dim: 128
  input_dim: 4
  state_dim: 4
  n_actions: 4
  act:
    _target_: torch.nn.ReLU
  
backward_model: null

partition_model:
  _target_: chunkgfn.models.rnn.logZ
  num_layers: 1
  hidden_dim: 128
  input_vocab_size: 4
  act:
    _target_: torch.nn.ReLU

epsilon_scheduler:
  _target_: chunkgfn.schedulers.linear_schedule.LinearSchedule
  initial_value: 1
  final_value: 0
  max_epochs: ${trainer.max_epochs}

replay_buffer:
  _target_: chunkgfn.replay_buffer.prioritized_replay.PrioritizedReplay
  capacity: 5000
  cutoff_distance: 5

forward_lr: 5e-4
partition_lr: 1e-4
monitor: val/logreward
ratio_from_replay_buffer: 0.55 # Ratio of samples from replay buffer
ratio_backward: 0.5 # Ratio of samples from backward model
n_trajectories: 20 # Number of trajectories to sample per input
reward_temperature: 1.0 # Temperature for the reward