_target_: chunkgfn.gfn.cond_tb_gfn.Cond_TBGFN

optimizer:
  _target_: torch.optim.Adam
  _partial_: true
  weight_decay: 0.0

scheduler: null

forward_model:
  _target_: chunkgfn.models.rnn.RNN
  num_layers: 1
  hidden_dim: 128
  input_vocab_size: 4
  state_vocab_size: 4
  act:
    _target_: torch.nn.ReLU
  
backward_model: null

partition_model:
  _target_: chunkgfn.models.rnn.logZ
  num_layers: 1
  hidden_dim: 128
  input_vocab_size: 4
  act:
    _target_: torch.nn.ReLU

criterion:
  _target_: chunkgfn.rewards.mse.MSE

epsilon_scheduler:
  _target_: chunkgfn.schedulers.linear_schedule.LinearSchedule
  initial_value: 1
  final_value: 0
  max_epochs: ${trainer.max_epochs}

replay_buffer:
  _target_: chunkgfn.replay_buffer.prioritized_replay.PrioritizedReplay
  capacity: 5000
  cutoff_distance: 5

lr: 3e-4
partition_lr: 5e-4
monitor: train/loss
ratio_from_replay_buffer: 0.55 # Ratio of samples from replay buffer
n_trajectories: 30 # Number of trajectories to sample per input
reward_temperature: 1.0 # Temperature for the reward