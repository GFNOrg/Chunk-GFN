# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:
  - override /data: selfies_sequence.yaml
  - override /gfn: a2c_prioritized.yaml
  - override /trainer: default.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

seed: 42

data:
  max_len: 10
  num_train_iterations: 25
  sample_exact_length: False
  batch_size: 256

trainer:
  max_epochs: 1500
  check_val_every_n_epoch: 20

gfn:
  forward_model:
    _target_: chunkgfn.models.rnn.UnconditionalRNN
    num_layers: 2
    hidden_dim: 256
    state_dim: 31
    action_embedding_dim: ${gfn.action_model.action_embedding_dimension}
    act:
      _target_: torch.nn.ReLU
  
  critic_model:
    _target_: chunkgfn.models.rnn.Critic
    num_layers: 2
    hidden_dim: 256
    state_dim: 31
    act:
      _target_: torch.nn.ReLU

  action_model:
    _target_: chunkgfn.models.action_encoder.ActionModel
    n_primitive_actions: 31

  replay_buffer:
    cutoff_distance: 3
    capacity: 10000
  reward_temperature: 0.3333
  

  forward_lr: 1e-3
  critic_lr: 1e-3
  action_lr: 1e-3

logger:
  wandb:
    group: "selfies_sequence"
