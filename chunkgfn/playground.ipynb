{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing the number of PBs \n",
    "import numpy as np\n",
    "def N(n, k):\n",
    "    '''\n",
    "    n : sequence length\n",
    "    k : max token length\n",
    "    '''\n",
    "    if n == 0:\n",
    "        return  1\n",
    "    else : \n",
    "        return np.sum([N(i,k) for i in range(n - k, n)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The n-bonacci formula : \n",
    "\n",
    "Let n be the number of letters in a string $S$, and suppose we have : \n",
    "- a complete library with all possible subtoken of S\n",
    "- This implies a GFN structure where all the parents \n",
    ". Let $N(S)$ be the number of backward trajectories that lead to $S$. Then : \n",
    "\n",
    "$$N(S) = \\sum_{i = 0}^{n-1} N(i) \\quad \\text{Where}  \\quad N(0) = 1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "def compute_logN( terminal_string, alpha, library):\n",
    "        ''' \n",
    "        Returns the weighted number of trajectories, according to Kolya's formula.\n",
    "\n",
    "        Args :\n",
    "            terminal_string : str, the terminal string\n",
    "            alpha : float, temperature parameter\n",
    "        Returns:\n",
    "            logN : dict, where:\n",
    "                - keys are all possible substates that can be used to construct terminal_string,\n",
    "                - Value is the log-weighted number of trajectories that go through that substate, given terminal_string.\n",
    "            mask_back_action : tensor of size len(terminal_string) x len(actions), where mask_back_action[i,j] is 1 if action j is a parent of terminal_string[:i]\n",
    "        '''    \n",
    "        atomic_traj = list(terminal_string.replace('<EOS>',''))\n",
    "        logN = {'': 0}\n",
    "        mask_back_action = torch.ones(len(terminal_string), len(library))  # Parents mask. Is 1 if the action is a parent of the current string\n",
    "        actions = { action: len(action) for action in library}\n",
    "        for i in range(1, len(atomic_traj) + 1):\n",
    "            parents_i = []\n",
    "            for action, j in actions.items():\n",
    "                if terminal_string[i - j  :i]  == action:\n",
    "                    mask_back_action[i - 1, library.index(action)] = 0\n",
    "                    parents_i.append(terminal_string[:i - j])\n",
    "\n",
    "            logN_parents_i = torch.Tensor([ logN[s] for s in parents_i])\n",
    "            logN[ terminal_string[:i] ] = alpha + torch.logsumexp(logN_parents_i, dim = 0).item()\n",
    "        return logN, mask_back_action\n",
    "\n",
    "\n",
    "\n",
    "def get_logpb_state(string, terminal_string, alpha, logN, mask_back_action, library):\n",
    "    '''\n",
    "    Computes the logpb of each action in the library given the current state ( = string), according to Kolya's formula.\n",
    "\n",
    "    Args :\n",
    "        string : str, the current state\n",
    "        terminal_string : str, the terminal state\n",
    "        alpha : float, temperature parameter\n",
    "        N : dict, the weighted number of trajectories given terminal_string\n",
    "        mask_back_action : tensor of size len(terminal_string) x len(actions), where mask_back_action[i,j] is 1 if action j is a parent of terminal_string[:i]\n",
    "    \n",
    "    Returns :\n",
    "        logpb : tensor of size len(actions), where logpb[j] is the logpb of choosing action j for the current state string.\n",
    "    '''\n",
    "    assert terminal_string[:len(string)] == string \n",
    "    assert list(logN.keys())[-1] == terminal_string.replace('<EOS>','')\n",
    "    logpb = - torch.ones(len(library))*float(\"Inf\")\n",
    "    if string[-5:] == '<EOS>':\n",
    "        logpb[0] = 1 # Action of removing the EOS\n",
    "    elif len(string) > 0:\n",
    "        mask = mask_back_action[len(string) - 1]\n",
    "        ixs = np.where(mask==0)[0]\n",
    "        for j in ixs:\n",
    "            logpb[j] = alpha + logN[string[:- len(library[j])]] - logN[string]\n",
    "    elif len(string) == 0:\n",
    "        logpb = torch.zeros(len(library))\n",
    "        # When no action is available, just fill with uniform because\n",
    "        # it won't be picked anyway in the backward_step.\n",
    "        # Doing this avoids having nan when computing probabilities\n",
    "    return logpb\n",
    "    \n",
    "\n",
    "def get_logpb_traj(trajectory, alpha, logN, library):\n",
    "    ''' \n",
    "    Computes the logpb of each action in the library given one trajectory, according to Kolya's formula. \n",
    "    Args : \n",
    "        traj : list of states, representing a trajectory sampled by a Gflownet\n",
    "        alpha : float, temperature parameter\n",
    "    Returns :\n",
    "        logpbs : tensor of size len(traj)-1 x len(actions), where logpbs[i,j] is the logpb of choosing action j for state i\n",
    "    '''\n",
    "    if trajectory[-1][-5:] == '<EOS>':\n",
    "        traj = trajectory[:-1]\n",
    "    else:\n",
    "        traj = trajectory\n",
    "    if list(logN.keys())[-1] != traj[-1] :\n",
    "        raise ValueError('The trajectory does not end with the terminal state for which logN was computed.')\n",
    "    logpbs = - torch.ones(len(traj[:-1]))*float(\"Inf\")\n",
    "    for i in range(1,len(traj)):\n",
    "        logpbs[i - 1] = alpha + logN[traj[i-1]] - logN[traj[i]]\n",
    "    return logpbs.sum().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) How slow is this, in terms of $L$ the library size and $T$ the string size ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Categorical\n",
    "\n",
    "def create_library(atomic_tokens, n_chunk, max_size_chunk):\n",
    "    #TODO : Add condition on n_chunk \n",
    "    library = atomic_tokens.copy()\n",
    "    i = 0\n",
    "    while i < n_chunk :\n",
    "        # Choose a length for the chunk\n",
    "        p = 0.5\n",
    "        logits = torch.Tensor([np.log(p)*i for i in range(max_size_chunk - 2)])\n",
    "        size_chunk = Categorical(logits = logits).sample() + 2\n",
    "        # Create a chunk \n",
    "        ixs = torch.randint(0, len(atomic_tokens), (size_chunk,))\n",
    "        chunk = ''.join([atomic_tokens[ix] for ix in ixs])\n",
    "        if chunk not in library:\n",
    "            library.append(chunk)\n",
    "            i +=1\n",
    "\n",
    "    return library\n",
    "\n",
    "def create_terminal_state(atomic_tokens, size):\n",
    "    terminal_state = ''\n",
    "    for _ in range(size):\n",
    "        ix = torch.randint(0, len(atomic_tokens), (1,))\n",
    "        terminal_state += atomic_tokens[ix]\n",
    "    return terminal_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "atomic_tokens = ['a', 'b', 'c', 'd']\n",
    "library = create_library(atomic_tokens = atomic_tokens, n_chunk = 10, max_size_chunk = 6)\n",
    "terminal_string = create_terminal_state(atomic_tokens, 20)\n",
    "alpha = -1\n",
    "logN, mask_back_action = compute_logN(terminal_string, alpha, library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 0,\n",
       " 'd': -1.0,\n",
       " 'dc': -2.0,\n",
       " 'dca': -1.68673837184906,\n",
       " 'dcab': -2.68673837184906,\n",
       " 'dcabd': -3.6867384910583496,\n",
       " 'dcabdb': -4.68673849105835,\n",
       " 'dcabdba': -5.68673849105835,\n",
       " 'dcabdbac': -6.68673849105835,\n",
       " 'dcabdbacd': -7.68673849105835,\n",
       " 'dcabdbacda': -7.373476982116699,\n",
       " 'dcabdbacdab': -8.3734769821167,\n",
       " 'dcabdbacdaba': -9.3734769821167,\n",
       " 'dcabdbacdabac': -10.3734769821167,\n",
       " 'dcabdbacdabacb': -11.3734769821167,\n",
       " 'dcabdbacdabacbb': -11.06021499633789,\n",
       " 'dcabdbacdabacbbc': -12.06021499633789,\n",
       " 'dcabdbacdabacbbcb': -13.06021499633789,\n",
       " 'dcabdbacdabacbbcbc': -14.06021499633789,\n",
       " 'dcabdbacdabacbbcbcc': -15.06021499633789,\n",
       " 'dcabdbacdabacbbcbccd': -16.06021499633789}"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0.051905155181884766 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "st = time.time()\n",
    "for i in range(64):\n",
    "    logN, mask_back_action = compute_logN(terminal_string, alpha, library)\n",
    "et = time.time()\n",
    "print('Execution time:', et - st, 'seconds')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 - How much does this PB help compared to uniform PB ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_uniform_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
